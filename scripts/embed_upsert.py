import os, json, glob, hashlib, time
from pathlib import Path
from typing import List, Dict

from tenacity import retry, wait_exponential, stop_after_attempt
from pymilvus import connections, Collection, utility, MilvusException
from openai import OpenAI

ROOT = Path(__file__).resolve().parents[1]
EXPORTS = ROOT / "data" / "exports"
LOGS = ROOT / "data" / "logs"

OPENAI_MODEL = os.getenv("OPENAI_EMBEDDING_MODEL", "text-embedding-3-large")
ZILLIZ_URI = os.getenv("ZILLIZ_URI")
ZILLIZ_TOKEN = os.getenv("ZILLIZ_TOKEN")

assert os.getenv("OPENAI_API_KEY"), "Missing OPENAI_API_KEY in environment"
assert ZILLIZ_URI and ZILLIZ_TOKEN, "Missing ZILLIZ_URI or ZILLIZ_TOKEN in .env"

client = OpenAI()

def batched(it, n=64):
    buf=[]
    for x in it:
        buf.append(x)
        if len(buf)>=n:
            yield buf; buf=[]
    if buf: yield buf

@retry(wait=wait_exponential(min=1, max=20), stop=stop_after_attempt(6))
def embed_texts(texts: List[str]) -> List[List[float]]:
    # OpenAI returns ordered embeddings for inputs
    resp = client.embeddings.create(model=OPENAI_MODEL, input=texts)
    return [e.embedding for e in resp.data]

def get_collection(name="brl_chunks") -> Collection:
    connections.connect(alias="default", uri=ZILLIZ_URI, token=ZILLIZ_TOKEN, timeout=30)
    return Collection(name)

def fingerprint(s: str) -> str:
    return hashlib.sha256(s.encode("utf-8")).hexdigest()

def already_loaded_hashes(col: Collection) -> set:
    # If you added a scalar unique 'hash', we can query existing to skip (optional).
    # For large sets this can be expensive; weâ€™ll just skip this step and rely on PK uniqueness.
    return set()

def record_to_row(r: Dict) -> Dict:
    # Ensure required fields exist; fill defaults
    return {
        "id": r["id"],
        "work_id": r["work_id"],
        "author": r.get("author",""),
        "work_title": r.get("work_title",""),
        "section_id": r.get("section_id",""),
        "paragraph_id": r.get("paragraph_id",""),
        "source_url": r.get("source_url",""),
        "lang": r.get("lang","en"),
        "created_at": int(time.time()),
        "hash": r.get("hash") or fingerprint(r["text"]),
        "text": r["text"],
        # text_sparse is generated by Zilliz full-text index; we don't send it during insert
        # "text_sparse": ...  # omit
        # "text_dense" added after embedding
    }

def upsert_rows(col: Collection, rows: List[Dict]):
    # Pymilvus accepts dict rows with all fields present
    try:
        col.insert(rows)
    except MilvusException as e:
        # Show a compact error
        raise

def main():
    col = get_collection()
    total = 0
    for path in sorted(glob.glob(str(EXPORTS / "*_children.jsonl"))):
        work_id = Path(path).name.replace("_children.jsonl","")
        print(f"==> Processing {work_id}")
        # Read JSONL
        records=[]
        with open(path, "r", encoding="utf-8") as f:
            for line in f:
                if not line.strip(): continue
                records.append(json.loads(line))
        # Batch embed and upsert
        for chunk in batched(records, n=64):
            texts = [r["text"] for r in chunk]
            embs = embed_texts(texts)
            rows=[]
            for r, e in zip(chunk, embs):
                row = record_to_row(r)
                row["text_dense"] = e
                rows.append(row)
            upsert_rows(col, rows)
            total += len(rows)
            print(f"   + upserted {len(rows)} (running total {total})")
    print(f"Done. Upserted {total} child chunks.")

if __name__=="__main__":
    main()
